{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import huggingface_hub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoConfig, T5Config\n",
    "from transformers import AutoTokenizer, T5TokenizerFast\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    "from transformers import AutoModel, T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Seq2SeqTrainingArguments\n",
    "from transformers import Trainer, Seq2SeqTrainer\n",
    "from transformers import pipeline\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset #, load_from_disk\n",
    "\n",
    "\n",
    "from mamba_ssm import Mamba\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from einops import rearrange\n",
    "\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 2 cpu:8\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_count = torch.cuda.device_count()\n",
    "# torch.cuda.empty_cache()\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "print(device, device_count, f'cpu:{cpu_cores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"state-spaces/mamba-2.8b\"\n",
    "modelHead_checkpoint_mamba = \"state-spaces/mamba-130m\"\n",
    "tokenizer_checkpoint_mamba = \"EleutherAI/gpt-neox-20b\"\n",
    "\n",
    "tokenizer_mamba = AutoTokenizer.from_pretrained(tokenizer_checkpoint_mamba)\n",
    "model_mamba = MambaLMHeadModel.from_pretrained(modelHead_checkpoint_mamba, device=device, dtype=torch.float16).to(device)\n",
    "# config = AutoConfig.from_pretrained(\"state-spaces/mamba-2.8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_t5 = \"t5-small\"\n",
    "\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(model_checkpoint_t5)\n",
    "# config_t5 = AutoConfig.from_pretrained(model_checkpoint_t5, output_hidden_states=True)\n",
    "# print(config_t5)\n",
    "model_t5 = AutoModel.from_pretrained(model_checkpoint_t5).to(device)\n",
    "# print(model_t5.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !touch .env\n",
    "# # !touch .gitignore\n",
    "# # !echo '.env' >> .gitignore\n",
    "# # !echo 'HUGGINGFACE_TOKEN=\"hf_ONAr\"' >> .env'\n",
    "# load_dotenv()\n",
    "# huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# # print(huggingface_token)\n",
    "# # huggingface_hub.login(token=huggingface_token, add_to_git_credential=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbtb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
