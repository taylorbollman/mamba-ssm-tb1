{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import lightning as L\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "import huggingface_hub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoConfig, T5Config\n",
    "from transformers import AutoTokenizer, T5TokenizerFast\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    "from transformers import AutoModel, T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Seq2SeqTrainingArguments\n",
    "from transformers import Trainer, Seq2SeqTrainer\n",
    "from transformers import pipeline\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset #, load_from_disk\n",
    "\n",
    "\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
    "from mamba_ssm import Mamba\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# find where in os path the token is stored:\n",
    "print(huggingface_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def prepare(rank, world_size, batch_size=32, pin_memory=False, num_workers=0):\n",
    "    dataset = dataset  # REFERENCE YOUR DATASET HERE!!\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank, world_size):\n",
    "    # setup the process groups\n",
    "    setup(rank, world_size)\n",
    "    # prepare the dataloader\n",
    "    dataloader = prepare(rank, world_size)\n",
    "    \n",
    "    # instantiate the model(it's your own model) and move it to the right device\n",
    "    model = model.to(rank)  # REFERENCE YOUR MODEL HERE!!\n",
    "    \n",
    "    # wrap the model with DDP\n",
    "    # device_ids tell DDP where is your model\n",
    "    # output_device tells DDP where to output, in our case, it is rank\n",
    "    # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model\n",
    "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distributed Data Parallel DDP with PyTorch Lightning -- EXAMPLE CODE for reference\n",
    "# for epoch in epochs:\n",
    "#     # if we are using DistributedSampler, we have to tell it which epoch this is\n",
    "#     dataloader.sampler.set_epoch(epoch)       \n",
    "    \n",
    "#     for step, x in enumerate(dataloader):\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "#         pred = model(x)\n",
    "#         label = x['label']\n",
    "        \n",
    "#         loss = loss_fn(pred, label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "# cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 0 cpu:4\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cuda_count = torch.cuda.device_count()\n",
    "torch.cuda.empty_cache()\n",
    "cpu_cores = mp.cpu_count()\n",
    "print(device, cuda_count, f'cpu:{cpu_cores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model_block_indep = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model_block_indep(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba(\n",
      "  (in_proj): Linear(in_features=16, out_features=64, bias=False)\n",
      "  (conv1d): Conv1d(32, 32, kernel_size=(4,), stride=(1,), padding=(3,), groups=32)\n",
      "  (act): SiLU()\n",
      "  (x_proj): Linear(in_features=32, out_features=33, bias=False)\n",
      "  (dt_proj): Linear(in_features=1, out_features=32, bias=True)\n",
      "  (out_proj): Linear(in_features=32, out_features=16, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_block_indep)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m tokenizer_checkpoint_mamba \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEleutherAI/gpt-neox-20b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m tokenizer_mamba \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_checkpoint_mamba)\n\u001b[0;32m----> 8\u001b[0m model_mamba \u001b[38;5;241m=\u001b[39m MambaLMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(modelHead_checkpoint_mamba, device\u001b[38;5;241m=\u001b[39m\u001b[43mdevice\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# config = AutoConfig.from_pretrained(\"state-spaces/mamba-2.8b\")\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "#### MAMBA MODEL Stuff ####\n",
    "\n",
    "# model_checkpoint = \"state-spaces/mamba-2.8b\"\n",
    "modelHead_checkpoint_mamba = \"state-spaces/mamba-130m\"\n",
    "tokenizer_checkpoint_mamba = \"EleutherAI/gpt-neox-20b\"\n",
    "\n",
    "tokenizer_mamba = AutoTokenizer.from_pretrained(tokenizer_checkpoint_mamba)\n",
    "model_mamba = MambaLMHeadModel.from_pretrained(modelHead_checkpoint_mamba, device=device, dtype=torch.float16).to(device)\n",
    "# config = AutoConfig.from_pretrained(\"state-spaces/mamba-2.8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MambaConfig(d_model=768, n_layer=24, vocab_size=50277, ssm_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8)\n",
      "Linear(in_features=768, out_features=50280, bias=False)\n",
      "MambaLMHeadModel(\n",
      "  (backbone): MixerModel(\n",
      "    (embedding): Embedding(50280, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Block(\n",
      "        (mixer): Mamba(\n",
      "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
      "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
      "          (act): SiLU()\n",
      "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
      "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
      "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        )\n",
      "        (norm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm_f): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (model_mamba.config)\n",
    "print(model_mamba.lm_head)\n",
    "print(model_mamba.modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### T5 MODEL Stuff ####\n",
    "\n",
    "# model_checkpoint_t5 = \"t5-small\"\n",
    "\n",
    "# tokenizer_t5 = AutoTokenizer.from_pretrained(model_checkpoint_t5)\n",
    "# # config_t5 = AutoConfig.from_pretrained(model_checkpoint_t5, output_hidden_states=True)\n",
    "# # print(config_t5)\n",
    "# model_t5 = AutoModel.from_pretrained(model_checkpoint_t5).to(device)\n",
    "# # print(model_t5.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR DISTRIBUTED DATAPARALLEL (DDP) -- FOR USE IN SCRIPT \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # suppose we have 3 gpus\n",
    "#     world_size = torch.cuda.device_count()  # number of GPUs \n",
    "#     mp.spawn(\n",
    "#         main,\n",
    "#         args=(world_size),\n",
    "#         nprocs=world_size\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbtb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
