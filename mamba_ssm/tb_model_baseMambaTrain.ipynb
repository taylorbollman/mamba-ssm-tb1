{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taylorbollman/yes/envs/for1013_mamba2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import lightning as L\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "import huggingface_hub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoConfig, T5Config\n",
    "from transformers import AutoTokenizer, T5TokenizerFast\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    "from transformers import AutoModel, T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Seq2SeqTrainingArguments\n",
    "from transformers import Trainer, Seq2SeqTrainer\n",
    "from transformers import pipeline\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
    "from mamba_ssm import Mamba\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1801350\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "raw_datasets[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1801350\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def setup(rank, world_size):\n",
    "#     os.environ['MASTER_ADDR'] = 'localhost'\n",
    "#     os.environ['MASTER_PORT'] = '12355'\n",
    "#     dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# def prepare(rank, world_size, batch_size=32, pin_memory=False, num_workers=0):\n",
    "#     dataset = dataset  # REFERENCE YOUR DATASET HERE!!\n",
    "#     sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    \n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)\n",
    "    \n",
    "#     return dataloader\n",
    "\n",
    "# def cleanup():\n",
    "#     dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(rank, world_size):\n",
    "#     # setup the process groups\n",
    "#     setup(rank, world_size)\n",
    "#     # prepare the dataloader\n",
    "#     dataloader = prepare(rank, world_size)\n",
    "    \n",
    "#     # instantiate the model(it's your own model) and move it to the right device\n",
    "#     model = model.to(rank)  # REFERENCE YOUR MODEL HERE!!\n",
    "    \n",
    "#     # wrap the model with DDP\n",
    "#     # device_ids tell DDP where is your model\n",
    "#     # output_device tells DDP where to output, in our case, it is rank\n",
    "#     # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model\n",
    "#     model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distributed Data Parallel DDP with PyTorch Lightning -- EXAMPLE CODE for reference\n",
    "# for epoch in epochs:\n",
    "#     # if we are using DistributedSampler, we have to tell it which epoch this is\n",
    "#     dataloader.sampler.set_epoch(epoch)       \n",
    "    \n",
    "#     for step, x in enumerate(dataloader):\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "#         pred = model(x)\n",
    "#         label = x['label']\n",
    "        \n",
    "#         loss = loss_fn(pred, label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "# cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 2 cpu:4\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cuda_count = torch.cuda.device_count()\n",
    "torch.cuda.empty_cache()\n",
    "cpu_cores = mp.cpu_count()\n",
    "print(device, cuda_count, f'cpu:{cpu_cores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, length, dim = 2, 64, 16\n",
    "# x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "# model_block_indep = Mamba(\n",
    "#     # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "#     d_model=dim, # Model dimension d_model\n",
    "#     d_state=16,  # SSM state expansion factor\n",
    "#     d_conv=4,    # Local convolution width\n",
    "#     expand=2,    # Block expansion factor\n",
    "# ).to(\"cuda\")\n",
    "# y = model_block_indep(x)\n",
    "# assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "# model = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba-2.8b-slimpj\", device=\"cuda\", dtype=torch.bfloat16)\n",
    "# tokens = tokenizer(\"Once upon a time, a cat named\", return_tensors=\"pt\")\n",
    "# input_ids = tokens.input_ids.to(device=\"cuda\")\n",
    "# max_length = input_ids.shape[1] + 80\n",
    "# fn = lambda: model.generate(\n",
    "#         input_ids=input_ids, max_length=max_length, cg=True,\n",
    "#         return_dict_in_generate=True, output_scores=True,\n",
    "#         enable_timing=False, temperature=0.9, top_k=40, top_p=0.9,)\n",
    "# out = fn()\n",
    "# print(tokenizer.decode(out[0][0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_block_indep)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "#### MAMBA MODEL Stuff ####\n",
    "\n",
    "# model_checkpoint = \"state-spaces/mamba-2.8b\"\n",
    "modelHead_checkpoint_mamba = \"state-spaces/mamba-130m\"\n",
    "# tokenizer_checkpoint_mamba = \"EleutherAI/gpt-neox-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", use_fast=True, num_proc=cpu_cores)\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint_mamba)\n",
    "model_mamba = MambaLMHeadModel.from_pretrained(modelHead_checkpoint_mamba, device=device, dtype=torch.float16).to(device)\n",
    "# config = AutoConfig.from_pretrained(\"state-spaces/mamba-2.8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=50280, bias=False)\n",
      "<bound method Module.modules of MambaLMHeadModel(\n",
      "  (backbone): MixerModel(\n",
      "    (embedding): Embedding(50280, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Block(\n",
      "        (mixer): Mamba(\n",
      "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
      "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
      "          (act): SiLU()\n",
      "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
      "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
      "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        )\n",
      "        (norm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm_f): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model_mamba.lm_head)\n",
    "print(model_mamba.modules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['', ' = Valkyria Chronicles III = \\n', '', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 4000/1801350 [00:00<03:56, 7599.49 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1801350/1801350 [03:13<00:00, 9308.06 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# watch -n 3 free -h\n",
    "context_length = 256\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(element):\n",
    "    # Tokenize the text and truncate to the desired max length\n",
    "    return tokenizer(element[\"text\"], max_length=context_length, truncation=True, padding=False)\n",
    "\n",
    "train_tokenized_datasets = raw_datasets[\"train\"].map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# def tokenize(batch):\n",
    "#     # Tokenize the batch of text and truncate to the desired max length\n",
    "#     return tokenizer(batch[\"text\"], max_length=context_length, truncation=True, return_tensors='pt', padding=False)\n",
    "\n",
    "# train_tokenized_datasets = raw_datasets[\"train\"].map(\n",
    "#     tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    "# )\n",
    "\n",
    "\n",
    "# def tokenize(batch):\n",
    "#     # Filter out empty strings to avoid issues during tokenization\n",
    "#     batch_texts = [text for text in batch[\"text\"] if text.strip()]\n",
    "    \n",
    "#     # Check if the batch is empty after filtering\n",
    "#     if not batch_texts:\n",
    "#         return {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "#     # Tokenize the filtered batch of text\n",
    "#     return tokenizer(batch_texts, max_length=context_length, truncation=True, return_tensors='pt')\n",
    "\n",
    "# train_tokenized_datasets = raw_datasets[\"train\"].map(\n",
    "#     tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # Concatenate the rows of the dataset\n",
    "# concatenated_text = ' '.join(raw_datasets[\"train\"][\"text\"])\n",
    "\n",
    "# # Determine an appropriate max_length\n",
    "# context_length = 256  # Example, adjust based on model's context window and your hardware\n",
    "\n",
    "# # Tokenize the concatenated text with the chosen max_length\n",
    "# tokens = tokenizer(concatenated_text, return_tensors='pt', max_length=context_length, truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "# Save the train_tokenized_datasets to disk:\n",
    "train_tokenized_datasets.save_to_disk(\"train_tokenized_datasets\")\n",
    "# Get size of this file:\n",
    "print(os.path.getsize(\"/home/taylorbollman/tbprojects1/mamba-ssm-tb1/mamba_ssm/wikitext-103-v1-tokenized\"))\n",
    "# Load the tokenized dataset from disk:\n",
    "train_tokenized_datasets = load_from_disk(\"/home/taylorbollman/tbprojects1/mamba-ssm-tb1/mamba_ssm/wikitext-103-v1-tokenized\")\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate the rows of the dataset\n",
    "# concatenated_text = ' '.join(raw_datasets[\"train\"][\"text\"])\n",
    "\n",
    "# # Tokenize the concatenated text\n",
    "# tokens = tokenizer(concatenated_text, return_tensors='pt', truncation=True)\n",
    "\n",
    "# # Prepare dataset for DataLoader\n",
    "# sequence_length = 512  # This is an example length, adjust based on your model and memory constraints\n",
    "# inputs = tokens.input_ids\n",
    "# inputs = inputs[:, :sequence_length]  # Truncate to the desired sequence length\n",
    "\n",
    "# # Create a PyTorch dataset and dataloader\n",
    "# dataset = TensorDataset(inputs)\n",
    "# dataloader = DataLoader(dataset, batch_size=8)  # Adjust batch size as needed\n",
    "\n",
    "# Now you can iterate over dataloader in your training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' = Valkyria Chronicles III = \\n',\n",
       " '',\n",
       " ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n',\n",
       " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\",\n",
       " \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\",\n",
       " '',\n",
       " ' = = Gameplay = = \\n',\n",
       " '',\n",
       " \" As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\"]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_datasets[\"train\"][\"text\"] = raw_datasets[\"train\"][\"text\"].str.replace(\"\\n\", \" \")\n",
    "# raw_datasets[\"validation\"][\"text\"] = raw_datasets[\"validation\"][\"text\"].str.replace(\"\\n\", \" \")\n",
    "# raw_datasets[\"test\"][\"text\"] = raw_datasets[\"test\"][\"text\"].str.replace(\"\\n\", \" \")\n",
    "raw_datasets[\"train\"][\"text\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 10\n",
      "Input chunk lengths: [0, 10, 0, 178, 113, 118, 0, 7, 0, 233]\n",
      "Chunk mapping: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "context_length = 1024\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:10][\"text\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "raw_datasets[\"train\"][\"text\"]\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### T5 MODEL Stuff ####\n",
    "\n",
    "# model_checkpoint_t5 = \"t5-small\"\n",
    "\n",
    "# tokenizer_t5 = AutoTokenizer.from_pretrained(model_checkpoint_t5)\n",
    "# # config_t5 = AutoConfig.from_pretrained(model_checkpoint_t5, output_hidden_states=True)\n",
    "# # print(config_t5)\n",
    "# model_t5 = AutoModel.from_pretrained(model_checkpoint_t5).to(device)\n",
    "# # print(model_t5.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR DISTRIBUTED DATAPARALLEL (DDP) -- FOR USE IN SCRIPT \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # suppose we have 3 gpus\n",
    "#     world_size = torch.cuda.device_count()  # number of GPUs \n",
    "#     mp.spawn(\n",
    "#         main,\n",
    "#         args=(world_size),\n",
    "#         nprocs=world_size\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbtb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
